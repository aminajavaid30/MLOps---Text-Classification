{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Assignment 1\n",
    "# Text Classification - **Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the file path where the trained model is saved\n",
    "model_file_path = \"naive_bayes_model.pkl\"\n",
    "\n",
    "# Load the saved Naive Bayes model from the file\n",
    "with open(model_file_path, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "print(\"Trained model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuations from text\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    regular_punct = string.punctuation\n",
    "    #return re.sub(r'[#!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~]', '', str(text))\n",
    "    return str(re.sub(r'['+regular_punct+']', '', str(text)))\n",
    "\n",
    "# Function to remove URLs from text\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http[s]?://\\S+', '', text)\n",
    "\n",
    "# Function to convert the text into lower case\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize(text):\n",
    "  wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  lemma_txt = ''\n",
    "  for w in tokens:\n",
    "    lemma_txt = lemma_txt + wordnet_lemmatizer.lemmatize(w) + ' '\n",
    "\n",
    "  return lemma_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Predict Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_data(data):\n",
    "    return tokenizer(data, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Convert tokenized data into tensors\n",
    "def convert_to_tensors(data):\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    \n",
    "    # Pad or truncate input_ids and attention_mask to ensure they have length 512\n",
    "    max_length = 512\n",
    "    padded_input_ids = torch.nn.functional.pad(input_ids, (0, max_length - input_ids.size(1)), value=tokenizer.pad_token_id)\n",
    "    padded_attention_mask = torch.nn.functional.pad(attention_mask, (0, max_length - attention_mask.size(1)), value=0)\n",
    "    \n",
    "    data = {'input_ids': padded_input_ids, 'attention_mask': padded_attention_mask}\n",
    "    return {key: torch.tensor(val) for key, val in data.items()}\n",
    "\n",
    "def predict_class(input_text, model):\n",
    "    # Preprocess input text\n",
    "    input_text = remove_punctuation(input_text)\n",
    "    input_text = remove_urls(input_text)\n",
    "    input_text = lower_case(input_text)\n",
    "    input_text = lemmatize(input_text)\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokenized_text = tokenize_data(str(input_text))\n",
    "    \n",
    "    # Convert tokenized data into tensors\n",
    "    data_tensors = convert_to_tensors(tokenized_text)\n",
    "    \n",
    "    # Convert PyTorch tensors to NumPy arrays\n",
    "    data_numpy = data_tensors['input_ids'].numpy()\n",
    "    \n",
    "    # Reshape the data to match Naive Bayes' input requirements\n",
    "    data_flattened = data_numpy.reshape(data_numpy.shape[0], -1)\n",
    "    \n",
    "    # Make predictions using the model\n",
    "    predicted_class = model.predict(data_flattened)\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [1]\n"
     ]
    }
   ],
   "source": [
    "# Sample Text 1:\n",
    "input_text = \"Great movie. I liked it!\"\n",
    "predicted_class = predict_class(input_text, loaded_model)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [0]\n"
     ]
    }
   ],
   "source": [
    "# Sample Text 2:\n",
    "input_text = \"I was extremely disappointed with this movie. The plot was predictable, the characters were one-dimensional, and the dialogue felt forced. It seemed like the filmmakers put more effort into the visuals than into crafting a compelling story. Overall, I found it to be a waste of time and money.\"\n",
    "predicted_class = predict_class(input_text, loaded_model)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [1]\n"
     ]
    }
   ],
   "source": [
    "# Sample Text 3:\n",
    "input_text = \"Good movie!\"\n",
    "predicted_class = predict_class(input_text, loaded_model)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [0]\n"
     ]
    }
   ],
   "source": [
    "# Sample Text 4:\n",
    "input_text = \"Extremely bad movie. I hate it!\"\n",
    "predicted_class = predict_class(input_text, loaded_model)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [0]\n"
     ]
    }
   ],
   "source": [
    "# Sample Text 5:\n",
    "input_text = \"I recently watched the latest Marvel movie, and I have to say, it was absolutely fantastic! From start to finish, the film kept me entertained with its thrilling action sequences, witty humor, and engaging storyline. The special effects were top-notch, and the performances by the cast were outstanding. I couldn't help but be immersed in the world of the movie, and I left the theater feeling exhilarated and wanting more. It's definitely a must-watch for any Marvel fan, and I can't wait to see it again!\"\n",
    "predicted_class = predict_class(input_text, loaded_model)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
